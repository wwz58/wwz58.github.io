---
title: HAN
tags: NLP
---
## Intro
HAN: 两级Attention（句子级别与词级别）机制的长文本分类网络模型

paper: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf

![模型结构图](https://raw.githubusercontent.com/wwz58/wwz58.github.io/master/assets/post/HAN/HAN.PNG)

## 主要思想
将一片文档分为 若干句子 每个句子由 若干词 组成

按照 词--attention--》得到句子表征向量--attention--》得到文档的表征向量--fc+softmax--》文档分类

构造模型，使用词级别的attention时 可以认为 每句话 有一个全局的向量表征Uw了 对应分类任务 所需的信息，将Uw与
句子中的每个词做attention，提取到 利于 分类任务的 句子表征向量。同理，在句子级别的attention时 可以认为 有一个全局的向量表征Us--句子级别的上下文信息向量，对应分类任务 所需的信息。将Us与
经过词级别attention得到的每个句子表征向量做attention，提取到这篇文档的表征向量。

## 模型搭建 
抽出self Attention层作为单独的功能块，在搭建整体网络时使用Attention

![网络搭建](https://raw.githubusercontent.com/wwz58/wwz58.github.io/master/assets/post/HAN/HAN_structure.jpg)

code

## 训练数据处理 
从模型输入 所需的数据形式 （模型输入： x: B s_l w_l longType）出发，构建DataSet类

- 首先是数据预处理 --》得到每篇文档以list of list表示的数据[[‘这是’， ‘句子’， ‘1’]， [‘这是’， ‘句子’， ‘2’]]

- 接着构建vocabulary， word2index, 根据 word2index 和下载的预训练词向量 构建在模型的Embedding层使用的embeding矩阵

- 接着从word2index， 预处理的文件， 构建自己的Dataset类

## 训练
注意训练过程中模型的保存
