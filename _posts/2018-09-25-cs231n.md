---
title: cs231n
tags: calss-notes
---
## Intro

540 million years ago eyes are developed and cause a boost in the number of animal species

cameras ---> eyes

H & W's experiement stimul and cat's primary context shows vision is hierachical and begins with simple edges 

people begin to solve vison tasks

previous methods fouce on feature and statistical machine learning model

1.recongnition everything in the world 2.machine learning model for vision tasks are complex , overfitting botleneck----> demands for 
large dataset

## knn

simply remember the train X-y set in the training process O(1), choose one distane metric to caculate the k neareset neighbours of the test image give it the lable which domanate the k neighbours. O(N) in the process of testing a new sample. distance function and k are hyperparameters here.

dataset slplit: train validation(tune and choose hyperparameters:distance function L1 or L2? k=?) test *cross validation* (split train+validation in folds,choose each of the folds as validation and others as train set and tune the parameters to see how can they affact the model, draw plot of the result including the mean and std is very useful)

## Linear Classifier

### basic idea:

strench the pixels in a image in a lng vector as input and find a linear function *f* which can map the input to classes scores ---> quality the f good or bad: loss function turn the problem into a optimazation problem----> get the f: optimazation backpropgation

### loss function SVM VS. softmax

the softmax loss can hardly be 0 s.t. it just keeps squash the loss but the svm can reach a minimum loss of 0

### the optimization process : backpropgation and compute graph

we do Gradient Desent to optimization

backpropgation: use chain rule recursively to get the loss's gradient with respect to every intermidiate variables. The gradient should have the same dimesion as the variable itself.

we can use compute graph to do backpropgation. the nodes in the graph represent computation and the links represent the data flow. we can build a node or composite several simple nodes into one node as long as we know how to caculate the gradient. This is important especially when we deal with big network structures.

in pratice we use Mini-batch SGD. interpreted as doing a mote-caro sampling to estimate the Gradient

the derive of svm loss / softmax loss / Wx / gradient:

### interpretation:

1. template

2. hyperplane


### Tips:

use numerical gradient to check the backpropgation implementation

the initial (first time forword propgation) softmax loss should be close to $-log(1/num_classes)$,use this to check the forword propgation implementation

## neural networks

### basic idea:

stack multiple linear layers together and insert non-linear layers in between, then forword and backword propgation to train the network.

### details:

choose the non-linear layer: re-lu

the re-lu backprop derivation:

[relu backprop]()

## CNN

### basic idea and inspriration

keep the spatial infomation


H & W's research about the hierachical vision system of cat. simple cells followed by more complicated cells

### how it works

convolution layer: input: $N*N*D$ hyperparameters: kernel $F*F*D S P K$  output: -->  $(N-F+2P)/S=N\_out$

num_of_parameters: $K*(F*F*D+1)$

conv-->(activation)---->relu-->pooling

### details

K usually powers of 2 eg:32,64,128...

F usually 1,3,5

S usually 1,2

P whatever matches
